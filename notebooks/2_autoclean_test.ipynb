{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import isnan\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoClean class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoClean:\n",
    "\n",
    "    def __init__(self, input_data, mode='auto', duplicates=False, missing_num=False, missing_categ=False, encode_categ=False, extract_datetime=False, outliers=False, outlier_param=1.5, verbose=False):  \n",
    "        output_data = input_data.copy()\n",
    "\n",
    "        if mode == 'auto':\n",
    "            duplicates, missing_num, missing_categ, outliers, encode_categ, extract_datetime = 'auto', 'auto', 'auto', 'winz', ['auto'], 's'\n",
    "\n",
    "        self.mode = mode\n",
    "        self.duplicates = duplicates\n",
    "        self.missing_num = missing_num\n",
    "        self.missing_categ = missing_categ\n",
    "        self.outliers = outliers\n",
    "        self.encode_categ = encode_categ\n",
    "        self.extract_datetime = extract_datetime\n",
    "        self.outlier_param = outlier_param\n",
    "        \n",
    "        # initialize our class and start the autoclean process\n",
    "        self.output = self._clean_data(output_data, input_data)  \n",
    "\n",
    "    def _clean_data(self, df, input_data):\n",
    "        # function for starting the autoclean process\n",
    "        df = df.reset_index(drop=True)\n",
    "        df = MissingValues.handle(self, df)\n",
    "        return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingValues:\n",
    "\n",
    "    self.strategy_map = {\n",
    "        'linreg': MissingValues._lin_regression_impute,\n",
    "        'knn': MissingValues._knn_impute,\n",
    "        'mean': MissingValues._simple_impute,\n",
    "        'median': MissingValues._simple_impute,\n",
    "        'most_frequent': MissingValues._simple_impute,\n",
    "        'delete': MissingValues._delete\n",
    "    }\n",
    "\n",
    "\n",
    "    def handle(self, df, _n_neighbors=3):\n",
    "        # function for handling missing values in the data\n",
    "        if not self.missing_num and not self.missing_categ:\n",
    "            return df\n",
    "\n",
    "        self.count_missing = df.isna().sum().sum()\n",
    "\n",
    "        if self.count_missing == 0:\n",
    "            return df\n",
    "\n",
    "        df = df.dropna(how='all')\n",
    "        df.reset_index(drop=True)\n",
    "        \n",
    "        if self.missing_num: # numeric data\n",
    "            # automated handling\n",
    "            if self.missing_num == 'auto': \n",
    "                imputer = SimpleImputer(strategy='median')\n",
    "                df = MissingValues._impute(self, df, imputer, type='num')\n",
    "            # mean, median or mode imputation\n",
    "            elif self.missing_num in ['mean', 'median', 'most_frequent']:\n",
    "                imputer = SimpleImputer(strategy=self.missing_num)\n",
    "                df = MissingValues._impute(self, df, imputer, type='num')\n",
    "            # delete missing values\n",
    "            elif self.missing_num == 'delete':\n",
    "                df = MissingValues._delete(self, df, type='num')\n",
    "\n",
    "        if self.missing_categ: # categorical data\n",
    "            # automated handling\n",
    "            if self.missing_categ == 'auto':\n",
    "                self.missing_categ = 'logreg'\n",
    "                lr = LogisticRegression()\n",
    "                df = MissingValues._log_regression_impute(self, df, lr)\n",
    "                self.missing_categ = 'knn'\n",
    "                imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
    "                df = MissingValues._impute(self, df, imputer, type='categ')\n",
    "            elif self.missing_categ == 'logreg':\n",
    "                lr = LogisticRegression()\n",
    "                df = MissingValues._log_regression_impute(self, df, lr)\n",
    "            # knn imputation\n",
    "            elif self.missing_categ == 'knn':\n",
    "                imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
    "                df = MissingValues._impute(self, df, imputer, type='categ')  \n",
    "            # mode imputation\n",
    "            elif self.missing_categ == 'most_frequent':\n",
    "                imputer = SimpleImputer(strategy=self.missing_categ)\n",
    "                df = MissingValues._impute(self, df, imputer, type='categ')\n",
    "            # delete missing values                    \n",
    "            elif self.missing_categ == 'delete':\n",
    "                df = MissingValues._delete(self, df, type='categ')\n",
    "        return df\n",
    "\n",
    "    def _impute(self, df, imputer, type):\n",
    "        # function for imputing missing values in the data\n",
    "        cols_num = df.select_dtypes(include=np.number).columns \n",
    "\n",
    "        if type == 'num':\n",
    "            # numerical features\n",
    "            for feature in df.columns: \n",
    "                if feature in cols_num:\n",
    "                    if df[feature].isna().sum().sum() != 0:\n",
    "                        try:\n",
    "                            df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)))\n",
    "                            counter = df[feature].isna().sum().sum() - df_imputed.isna().sum().sum()\n",
    "\n",
    "                            if (df[feature].fillna(-9999) % 1  == 0).all():\n",
    "                                df[feature] = df_imputed\n",
    "                                # round back to INTs, if original data were INTs\n",
    "                                df[feature] = df[feature].round()\n",
    "                                df[feature] = df[feature].astype('Int64')                                        \n",
    "                            else:\n",
    "                                df[feature] = df_imputed\n",
    "                            if counter != 0:\n",
    "                                print(\"imputation of value(s) succeeded for feature \")\n",
    "                        except:\n",
    "                            print(\"imputation failed for feature\")\n",
    "        else:\n",
    "            # categorical features\n",
    "            for feature in df.columns:\n",
    "                if feature not in cols_num:\n",
    "                    if df[feature].isna().sum()!= 0:\n",
    "                        try:\n",
    "                            mapping = dict()\n",
    "                            mappings = {k: i for i, k in enumerate(df[feature].dropna().unique(), 0)}\n",
    "                            mapping[feature] = mappings\n",
    "                            df[feature] = df[feature].map(mapping[feature])\n",
    "\n",
    "                            df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)), columns=[feature])    \n",
    "                            counter = sum(1 for i, j in zip(list(df_imputed[feature]), list(df[feature])) if i != j)\n",
    "\n",
    "                            # round to integers before mapping back to original values\n",
    "                            df[feature] = df_imputed\n",
    "                            df[feature] = df[feature].round()\n",
    "                            df[feature] = df[feature].astype('Int64')  \n",
    "\n",
    "                            # map values back to original\n",
    "                            mappings_inv = {v: k for k, v in mapping[feature].items()}\n",
    "                            df[feature] = df[feature].map(mappings_inv)\n",
    "                        except:\n",
    "                            print(\"imputation failed for feature \")\n",
    "        return df\n",
    "\n",
    "    def _delete(self, df, type):\n",
    "        # function for deleting missing values\n",
    "        cols_num = df.select_dtypes(include=np.number).columns \n",
    "        if type == 'num':\n",
    "            # numerical features\n",
    "            for feature in df.columns: \n",
    "                if feature in cols_num:\n",
    "                    df = df.dropna(subset=[feature])\n",
    "                    df.reset_index(drop=True)\n",
    "        else:\n",
    "            # categorical features\n",
    "            for feature in df.columns:\n",
    "                if feature not in cols_num:\n",
    "                    df = df.dropna(subset=[feature])\n",
    "                    df.reset_index(drop=True)\n",
    "        return df                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MissingValues_v0:\n",
    "    def handle(self, df, _n_neighbors=3):\n",
    "        # function for handling missing values in the data\n",
    "        if not self.missing_num and not self.missing_categ:\n",
    "            return df\n",
    "\n",
    "        self.count_missing = df.isna().sum().sum()\n",
    "\n",
    "        if self.count_missing == 0:\n",
    "            return df\n",
    "\n",
    "        df = df.dropna(how='all')\n",
    "        df.reset_index(drop=True)\n",
    "        \n",
    "        if self.missing_num: # numeric data\n",
    "            # automated handling\n",
    "            if self.missing_num == 'auto': \n",
    "                self.missing_num = 'linreg'\n",
    "                lr = LinearRegression()\n",
    "                df = MissingValues._lin_regression_impute(self, df, lr)\n",
    "                self.missing_num = 'knn'\n",
    "                imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
    "                df = MissingValues._impute(self, df, imputer, type='num')\n",
    "            # linear regression imputation\n",
    "            elif self.missing_num == 'linreg':\n",
    "                lr = LinearRegression()\n",
    "                df = MissingValues._lin_regression_impute(self, df, lr)\n",
    "            # knn imputation\n",
    "            elif self.missing_num == 'knn':\n",
    "                imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
    "                df = MissingValues._impute(self, df, imputer, type='num')\n",
    "            # mean, median or mode imputation\n",
    "            elif self.missing_num in ['mean', 'median', 'most_frequent']:\n",
    "                imputer = SimpleImputer(strategy=self.missing_num)\n",
    "                df = MissingValues._impute(self, df, imputer, type='num')\n",
    "            # delete missing values\n",
    "            elif self.missing_num == 'delete':\n",
    "                df = MissingValues._delete(self, df, type='num')\n",
    "\n",
    "        if self.missing_categ: # categorical data\n",
    "            # automated handling\n",
    "            if self.missing_categ == 'auto':\n",
    "                self.missing_categ = 'logreg'\n",
    "                lr = LogisticRegression()\n",
    "                df = MissingValues._log_regression_impute(self, df, lr)\n",
    "                self.missing_categ = 'knn'\n",
    "                imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
    "                df = MissingValues._impute(self, df, imputer, type='categ')\n",
    "            elif self.missing_categ == 'logreg':\n",
    "                lr = LogisticRegression()\n",
    "                df = MissingValues._log_regression_impute(self, df, lr)\n",
    "            # knn imputation\n",
    "            elif self.missing_categ == 'knn':\n",
    "                imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
    "                df = MissingValues._impute(self, df, imputer, type='categ')  \n",
    "            # mode imputation\n",
    "            elif self.missing_categ == 'most_frequent':\n",
    "                imputer = SimpleImputer(strategy=self.missing_categ)\n",
    "                df = MissingValues._impute(self, df, imputer, type='categ')\n",
    "            # delete missing values                    \n",
    "            elif self.missing_categ == 'delete':\n",
    "                df = MissingValues._delete(self, df, type='categ')\n",
    "        return df\n",
    "\n",
    "    def _impute(self, df, imputer, type):\n",
    "        # function for imputing missing values in the data\n",
    "        cols_num = df.select_dtypes(include=np.number).columns \n",
    "\n",
    "        if type == 'num':\n",
    "            # numerical features\n",
    "            for feature in df.columns: \n",
    "                if feature in cols_num:\n",
    "                    if df[feature].isna().sum().sum() != 0:\n",
    "                        try:\n",
    "                            df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)))\n",
    "                            counter = df[feature].isna().sum().sum() - df_imputed.isna().sum().sum()\n",
    "\n",
    "                            if (df[feature].fillna(-9999) % 1  == 0).all():\n",
    "                                df[feature] = df_imputed\n",
    "                                # round back to INTs, if original data were INTs\n",
    "                                df[feature] = df[feature].round()\n",
    "                                df[feature] = df[feature].astype('Int64')                                        \n",
    "                            else:\n",
    "                                df[feature] = df_imputed\n",
    "                            if counter != 0:\n",
    "                                print(\"imputation of value(s) succeeded for feature \")\n",
    "                        except:\n",
    "                            print(\"imputation failed for feature\")\n",
    "        else:\n",
    "            # categorical features\n",
    "            for feature in df.columns:\n",
    "                if feature not in cols_num:\n",
    "                    if df[feature].isna().sum()!= 0:\n",
    "                        try:\n",
    "                            mapping = dict()\n",
    "                            mappings = {k: i for i, k in enumerate(df[feature].dropna().unique(), 0)}\n",
    "                            mapping[feature] = mappings\n",
    "                            df[feature] = df[feature].map(mapping[feature])\n",
    "\n",
    "                            df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)), columns=[feature])    \n",
    "                            counter = sum(1 for i, j in zip(list(df_imputed[feature]), list(df[feature])) if i != j)\n",
    "\n",
    "                            # round to integers before mapping back to original values\n",
    "                            df[feature] = df_imputed\n",
    "                            df[feature] = df[feature].round()\n",
    "                            df[feature] = df[feature].astype('Int64')  \n",
    "\n",
    "                            # map values back to original\n",
    "                            mappings_inv = {v: k for k, v in mapping[feature].items()}\n",
    "                            df[feature] = df[feature].map(mappings_inv)\n",
    "                        except:\n",
    "                            print(\"imputation failed for feature \")\n",
    "        return df\n",
    "\n",
    "    def _lin_regression_impute(self, df, model):\n",
    "        # function for predicting missing values with linear regression\n",
    "        cols_num = df.select_dtypes(include=np.number).columns\n",
    "        mapping = dict()\n",
    "        for feature in df.columns:\n",
    "            if feature not in cols_num:\n",
    "                # create label mapping for categorical feature values\n",
    "                mappings = {k: i for i, k in enumerate(df[feature])}\n",
    "                mapping[feature] = mappings\n",
    "                df[feature] = df[feature].map(mapping[feature])\n",
    "        for feature in cols_num: \n",
    "                try:\n",
    "                    test_df = df[df[feature].isnull()==True].dropna(subset=[x for x in df.columns if x != feature])\n",
    "                    train_df = df[df[feature].isnull()==False].dropna(subset=[x for x in df.columns if x != feature])\n",
    "                    if len(test_df.index) != 0:\n",
    "                        pipe = make_pipeline(StandardScaler(), model)\n",
    "\n",
    "                        y = np.log(train_df[feature]) # log-transform the data\n",
    "                        X_train = train_df.drop(feature, axis=1)\n",
    "                        test_df.drop(feature, axis=1, inplace=True)\n",
    "                        \n",
    "                        try:\n",
    "                            model = pipe.fit(X_train, y)\n",
    "                        except:\n",
    "                            y = train_df[feature] # use non-log-transformed data\n",
    "                            model = pipe.fit(X_train, y)\n",
    "                        if (y == train_df[feature]).all():\n",
    "                            pred = model.predict(test_df)\n",
    "                        else:\n",
    "                            pred = np.exp(model.predict(test_df)) # predict values\n",
    "\n",
    "                        test_df[feature]= pred\n",
    "\n",
    "                        if (df[feature].fillna(-9999) % 1  == 0).all():\n",
    "                            # round back to INTs, if original data were INTs\n",
    "                            test_df[feature] = test_df[feature].round()\n",
    "                            test_df[feature] = test_df[feature].astype('Int64')\n",
    "                            df[feature].update(test_df[feature])                          \n",
    "                        else:\n",
    "                            df[feature].update(test_df[feature])  \n",
    "                except:\n",
    "                    print('LINREG imputation failed for feature')\n",
    "        for feature in df.columns: \n",
    "            try:   \n",
    "                # map categorical feature values back to original\n",
    "                mappings_inv = {v: k for k, v in mapping[feature].items()}\n",
    "                df[feature] = df[feature].map(mappings_inv)\n",
    "            except:\n",
    "                pass\n",
    "        return df\n",
    "\n",
    "    def _log_regression_impute(self, df, model):\n",
    "        # function for predicting missing values with logistic regression\n",
    "        cols_num = df.select_dtypes(include=np.number).columns\n",
    "        mapping = dict()\n",
    "        for feature in df.columns:\n",
    "            if feature not in cols_num:\n",
    "                # create label mapping for categorical feature values\n",
    "                mappings = {k: i for i, k in enumerate(df[feature])} #.dropna().unique(), 0)}\n",
    "                mapping[feature] = mappings\n",
    "                df[feature] = df[feature].map(mapping[feature])\n",
    "\n",
    "        target_cols = [x for x in df.columns if x not in cols_num]\n",
    "            \n",
    "        for feature in df.columns: \n",
    "            if feature in target_cols:\n",
    "                try:\n",
    "                    test_df = df[df[feature].isnull()==True].dropna(subset=[x for x in df.columns if x != feature])\n",
    "                    train_df = df[df[feature].isnull()==False].dropna(subset=[x for x in df.columns if x != feature])\n",
    "                    if len(test_df.index) != 0:\n",
    "                        pipe = make_pipeline(StandardScaler(), model)\n",
    "\n",
    "                        y = train_df[feature]\n",
    "                        train_df.drop(feature, axis=1, inplace=True)\n",
    "                        test_df.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "                        model = pipe.fit(train_df, y)\n",
    "                        \n",
    "                        pred = model.predict(test_df) # predict values\n",
    "                        test_df[feature]= pred\n",
    "\n",
    "                        if (df[feature].fillna(-9999) % 1  == 0).all():\n",
    "                            # round back to INTs, if original data were INTs\n",
    "                            test_df[feature] = test_df[feature].round()\n",
    "                            test_df[feature] = test_df[feature].astype('Int64')\n",
    "                            df[feature].update(test_df[feature])                             \n",
    "                except:\n",
    "                    print('LOGREG imputation failed for feature ')\n",
    "        for feature in df.columns: \n",
    "            try:\n",
    "                # map categorical feature values back to original\n",
    "                mappings_inv = {v: k for k, v in mapping[feature].items()}\n",
    "                df[feature] = df[feature].map(mappings_inv)\n",
    "            except:\n",
    "                pass     \n",
    "        return df\n",
    "\n",
    "    def _delete(self, df, type):\n",
    "        # function for deleting missing values\n",
    "        cols_num = df.select_dtypes(include=np.number).columns \n",
    "        if type == 'num':\n",
    "            # numerical features\n",
    "            for feature in df.columns: \n",
    "                if feature in cols_num:\n",
    "                    df = df.dropna(subset=[feature])\n",
    "                    df.reset_index(drop=True)\n",
    "        else:\n",
    "            # categorical features\n",
    "            for feature in df.columns:\n",
    "                if feature not in cols_num:\n",
    "                    df = df.dropna(subset=[feature])\n",
    "                    df.reset_index(drop=True)\n",
    "        return df                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class Outliers:\n",
    "\n",
    "    def handle(self, df):\n",
    "        # function for handling of outliers in the data\n",
    "        if self.outliers:\n",
    "            logger.info('Started handling of outliers... Method: \"{}\"', str(self.outliers).upper())\n",
    "            start = timer()  \n",
    "\n",
    "            if self.outliers in ['auto', 'winz']:  \n",
    "                df = Outliers._winsorization(self, df)\n",
    "            elif self.outliers == 'delete':\n",
    "                df = Outliers._delete(self, df)\n",
    "            \n",
    "            end = timer()\n",
    "            logger.info('Completed handling of outliers in {} seconds', round(end-start, 6))\n",
    "        else:\n",
    "            logger.info('Skipped handling of outliers')\n",
    "        return df     \n",
    "\n",
    "    def _winsorization(self, df):\n",
    "        # function for outlier winsorization\n",
    "        cols_num = df.select_dtypes(include=np.number).columns    \n",
    "        for feature in cols_num:           \n",
    "            counter = 0\n",
    "            # compute outlier bounds\n",
    "            lower_bound, upper_bound = Outliers._compute_bounds(self, df, feature)    \n",
    "            for row_index, row_val in enumerate(df[feature]):\n",
    "                if row_val < lower_bound or row_val > upper_bound:\n",
    "                    if row_val < lower_bound:\n",
    "                        if (df[feature].fillna(-9999) % 1  == 0).all():\n",
    "                                df.loc[row_index, feature] = lower_bound\n",
    "                                df[feature] = df[feature].astype(int) \n",
    "                        else:    \n",
    "                            df.loc[row_index, feature] = lower_bound\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        if (df[feature].fillna(-9999) % 1  == 0).all():\n",
    "                            df.loc[row_index, feature] = upper_bound\n",
    "                            df[feature] = df[feature].astype(int) \n",
    "                        else:\n",
    "                            df.loc[row_index, feature] = upper_bound\n",
    "                        counter += 1\n",
    "            if counter != 0:\n",
    "                logger.debug('Outlier imputation of {} value(s) succeeded for feature \"{}\"', counter, feature)        \n",
    "        return df\n",
    "\n",
    "    def _delete(self, df):\n",
    "        # function for deleting outliers in the data\n",
    "        cols_num = df.select_dtypes(include=np.number).columns    \n",
    "        for feature in cols_num:\n",
    "            counter = 0\n",
    "            lower_bound, upper_bound = Outliers._compute_bounds(self, df, feature)    \n",
    "            # delete observations containing outliers            \n",
    "            for row_index, row_val in enumerate(df[feature]):\n",
    "                if row_val < lower_bound or row_val > upper_bound:\n",
    "                    df = df.drop(row_index)\n",
    "                    counter +=1\n",
    "            df = df.reset_index(drop=True)\n",
    "            if counter != 0:\n",
    "                logger.debug('Deletion of {} outliers succeeded for feature \"{}\"', counter, feature)\n",
    "        return df\n",
    "\n",
    "    def _compute_bounds(self, df, feature):\n",
    "        # function that computes the lower and upper bounds for finding outliers in the data\n",
    "        featureSorted = sorted(df[feature])\n",
    "        \n",
    "        q1, q3 = np.percentile(featureSorted, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        lb = q1 - (self.outlier_param * iqr) \n",
    "        ub = q3 + (self.outlier_param * iqr) \n",
    "\n",
    "        return lb, ub    \n",
    "\n",
    "class Adjust:\n",
    "\n",
    "    def convert_datetime(self, df):\n",
    "        # function for extracting of datetime values in the data\n",
    "        if self.extract_datetime:\n",
    "            logger.info('Started conversion of DATETIME features... Granularity: {}', self.extract_datetime)\n",
    "            start = timer()\n",
    "            cols = set(df.columns) ^ set(df.select_dtypes(include=np.number).columns) \n",
    "            for feature in cols: \n",
    "                try:\n",
    "                    # convert features encoded as strings to type datetime ['D','M','Y','h','m','s']\n",
    "                    df[feature] = pd.to_datetime(df[feature], infer_datetime_format=True)\n",
    "                    try:\n",
    "                        df['Day'] = pd.to_datetime(df[feature]).dt.day\n",
    "\n",
    "                        if self.extract_datetime in ['auto', 'M','Y','h','m','s']:\n",
    "                            df['Month'] = pd.to_datetime(df[feature]).dt.month\n",
    "\n",
    "                            if self.extract_datetime in ['auto', 'Y','h','m','s']:\n",
    "                                df['Year'] = pd.to_datetime(df[feature]).dt.year\n",
    "\n",
    "                                if self.extract_datetime in ['auto', 'h','m','s']:\n",
    "                                    df['Hour'] = pd.to_datetime(df[feature]).dt.hour\n",
    "\n",
    "                                    if self.extract_datetime in ['auto', 'm','s']:\n",
    "                                        df['Minute'] = pd.to_datetime(df[feature]).dt.minute\n",
    "\n",
    "                                        if self.extract_datetime in ['auto', 's']:\n",
    "                                            df['Sec'] = pd.to_datetime(df[feature]).dt.second\n",
    "                        \n",
    "                        logger.debug('Conversion to DATETIME succeeded for feature \"{}\"', feature)\n",
    "\n",
    "                        try: \n",
    "                            # check if entries for the extracted dates/times are non-NULL, otherwise drop\n",
    "                            if (df['Hour'] == 0).all() and (df['Minute'] == 0).all() and (df['Sec'] == 0).all():\n",
    "                                df.drop('Hour', inplace = True, axis =1 )\n",
    "                                df.drop('Minute', inplace = True, axis =1 )\n",
    "                                df.drop('Sec', inplace = True, axis =1 )\n",
    "                            elif (df['Day'] == 0).all() and (df['Month'] == 0).all() and (df['Year'] == 0).all():\n",
    "                                df.drop('Day', inplace = True, axis =1 )\n",
    "                                df.drop('Month', inplace = True, axis =1 )\n",
    "                                df.drop('Year', inplace = True, axis =1 )   \n",
    "                        except:\n",
    "                            pass          \n",
    "                    except:\n",
    "                        # feature cannot be converted to datetime\n",
    "                        logger.warning('Conversion to DATETIME failed for \"{}\"', feature)\n",
    "                except:\n",
    "                    pass\n",
    "            end = timer()\n",
    "            logger.info('Completed conversion of DATETIME features in {} seconds', round(end-start, 4))\n",
    "        else:\n",
    "            logger.info('Skipped datetime feature conversion')\n",
    "        return df\n",
    "\n",
    "    def round_values(self, df, input_data):\n",
    "        # function that checks datatypes of features and converts them if necessary\n",
    "        if self.duplicates or self.missing_num or self.missing_categ or self.outliers or self.encode_categ or self.extract_datetime:\n",
    "            logger.info('Started feature type conversion...')\n",
    "            start = timer()\n",
    "            counter = 0\n",
    "            cols_num = df.select_dtypes(include=np.number).columns\n",
    "            for feature in cols_num:\n",
    "                    # check if all values are integers\n",
    "                    if (df[feature].fillna(-9999) % 1  == 0).all():\n",
    "                        try:\n",
    "                            # encode FLOATs with only 0 as decimals to INT\n",
    "                            df[feature] = df[feature].astype('Int64')\n",
    "                            counter += 1\n",
    "                            logger.debug('Conversion to type INT succeeded for feature \"{}\"', feature)\n",
    "                        except:\n",
    "                            logger.warning('Conversion to type INT failed for feature \"{}\"', feature)\n",
    "                    else:\n",
    "                        try:\n",
    "                            df[feature] = df[feature].astype(float)\n",
    "                            # round the number of decimals of FLOATs back to original\n",
    "                            dec = None\n",
    "                            for value in input_data[feature]:\n",
    "                                try:\n",
    "                                    if dec == None:\n",
    "                                        dec = str(value)[::-1].find('.')\n",
    "                                    else:\n",
    "                                        if str(value)[::-1].find('.') > dec:\n",
    "                                            dec = str(value)[::-1].find('.')\n",
    "                                except:\n",
    "                                    pass\n",
    "                            df[feature] = df[feature].round(decimals = dec)\n",
    "                            counter += 1\n",
    "                            logger.debug('Conversion to type FLOAT succeeded for feature \"{}\"', feature)\n",
    "                        except:\n",
    "                            logger.warning('Conversion to type FLOAT failed for feature \"{}\"', feature)\n",
    "            end = timer()\n",
    "            logger.info('Completed feature type conversion for {} feature(s) in {} seconds', counter, round(end-start, 6))\n",
    "        else:\n",
    "            logger.info('Skipped feature type conversion')\n",
    "        return df\n",
    "\n",
    "class EncodeCateg:\n",
    "\n",
    "    def handle(self, df):\n",
    "        # function for encoding of categorical features in the data\n",
    "        if self.encode_categ:\n",
    "            if not isinstance(self.encode_categ, list):\n",
    "                self.encode_categ = ['auto']\n",
    "            # select non numeric features\n",
    "            cols_categ = set(df.columns) ^ set(df.select_dtypes(include=np.number).columns) \n",
    "            # check if all columns should be encoded\n",
    "            if len(self.encode_categ) == 1:\n",
    "                target_cols = cols_categ # encode ALL columns\n",
    "            else:\n",
    "                target_cols = self.encode_categ[1] # encode only specific columns\n",
    "            logger.info('Started encoding categorical features... Method: \"{}\"', str(self.encode_categ[0]).upper())\n",
    "            start = timer()\n",
    "            for feature in target_cols:\n",
    "                if feature in cols_categ:\n",
    "                    # columns are column names\n",
    "                    feature = feature\n",
    "                else:\n",
    "                    # columns are indexes\n",
    "                    feature = df.columns[feature]\n",
    "                try:\n",
    "                    # skip encoding of datetime features\n",
    "                    pd.to_datetime(df[feature])\n",
    "                    logger.debug('Skipped encoding for DATETIME feature \"{}\"', feature)\n",
    "                except:\n",
    "                    try:\n",
    "                        if self.encode_categ[0] == 'auto':\n",
    "                            # ONEHOT encode if not more than 10 unique values to encode\n",
    "                            if df[feature].nunique() <=10:\n",
    "                                df = EncodeCateg._to_onehot(self, df, feature)\n",
    "                                logger.debug('Encoding to ONEHOT succeeded for feature \"{}\"', feature)\n",
    "                            # LABEL encode if not more than 20 unique values to encode\n",
    "                            elif df[feature].nunique() <=20:\n",
    "                                df = EncodeCateg._to_label(self, df, feature)\n",
    "                                logger.debug('Encoding to LABEL succeeded for feature \"{}\"', feature)\n",
    "                            # skip encoding if more than 20 unique values to encode\n",
    "                            else:\n",
    "                                logger.debug('Encoding skipped for feature \"{}\"', feature)   \n",
    "\n",
    "                        elif self.encode_categ[0] == 'onehot':\n",
    "                            df = EncodeCateg._to_onehot(df, feature)\n",
    "                            logger.debug('Encoding to {} succeeded for feature \"{}\"', str(self.encode_categ[0]).upper(), feature)\n",
    "                        elif self.encode_categ[0] == 'label':\n",
    "                            df = EncodeCateg._to_label(df, feature)\n",
    "                            logger.debug('Encoding to {} succeeded for feature \"{}\"', str(self.encode_categ[0]).upper(), feature)      \n",
    "                    except:\n",
    "                        logger.warning('Encoding to {} failed for feature \"{}\"', str(self.encode_categ[0]).upper(), feature)    \n",
    "            end = timer()\n",
    "            logger.info('Completed encoding of categorical features in {} seconds', round(end-start, 6))\n",
    "        else:\n",
    "            logger.info('Skipped encoding of categorical features')\n",
    "        return df\n",
    "\n",
    "    def _to_onehot(self, df, feature, limit=10):  \n",
    "        # function that encodes categorical features to OneHot encodings    \n",
    "        one_hot = pd.get_dummies(df[feature], prefix=feature)\n",
    "        if one_hot.shape[1] > limit:\n",
    "            logger.warning('ONEHOT encoding for feature \"{}\" creates {} new features. Consider LABEL encoding instead.', feature, one_hot.shape[1])\n",
    "        # join the encoded df\n",
    "        df = df.join(one_hot)\n",
    "        return df\n",
    "\n",
    "    def _to_label(self, df, feature):\n",
    "        # function that encodes categorical features to label encodings \n",
    "        le = preprocessing.LabelEncoder()\n",
    "\n",
    "        df[feature + '_lab'] = le.fit_transform(df[feature].values)\n",
    "        mapping = dict(zip(le.classes_, range(len(le.classes_))))\n",
    "        \n",
    "        for key in mapping:\n",
    "            try:\n",
    "                if isnan(key):               \n",
    "                    replace = {mapping[key] : key }\n",
    "                    df[feature].replace(replace, inplace=True)\n",
    "            except:\n",
    "                pass\n",
    "        return df  \n",
    "\n",
    "class Duplicates:\n",
    "\n",
    "    def handle(self, df):\n",
    "        if self.duplicates:\n",
    "            logger.info('Started handling of duplicates... Method: \"{}\"', str(self.duplicates).upper())\n",
    "            start = timer()\n",
    "            original = df.shape\n",
    "            try:\n",
    "                df.drop_duplicates(inplace=True, ignore_index=False)\n",
    "                df = df.reset_index(drop=True)\n",
    "                new = df.shape\n",
    "                count = original[0] - new[0]\n",
    "                if count != 0:\n",
    "                    logger.debug('Deletion of {} duplicate(s) succeeded', count)\n",
    "                else:\n",
    "                    logger.debug('{} missing values found', count)\n",
    "                end = timer()\n",
    "                logger.info('Completed handling of duplicates in {} seconds', round(end-start, 6))\n",
    "\n",
    "            except:\n",
    "                logger.warning('Handling of duplicates failed')        \n",
    "        else:\n",
    "            logger.info('Skipped handling of duplicates')\n",
    "        return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vent = df_vent[['Racha Max (Km/h)', 'Veloc. Media (Km/h)']]\n",
    "df_vent = df_vent[:20]\n",
    "print(df_vent[:2])\n",
    "# Add missing data\n",
    "missing_rows_indices = np.random.choice(df_vent.index, 10, replace=False)\n",
    "df_vent_with_missing = df_vent.copy()\n",
    "df_vent_with_missing.loc[missing_rows_indices, :] = np.nan\n",
    "df_vent_with_missing[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoClean( df_vent_with_missing )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('nasapp_3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae4bf1b7b32e9fe95d45ac2482af43ea1e0746da997778d9031d0eeed75cd7ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
